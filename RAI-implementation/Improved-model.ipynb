{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    23364\n",
      "1     6636\n",
      "Name: default payment next month, dtype: int64\n",
      "(30000, 25)\n"
     ]
    }
   ],
   "source": [
    "#Improved model created by analyzing and using the RAI principles\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import xgboost as xgb\n",
    "from packaging import version\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "#import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "'''print(tf.__version__)\n",
    "print(tf.executing_eagerly())\n",
    "tf.config.run_functions_eagerly(True)'''\n",
    "\n",
    "df = pd.read_excel(\"/home/jui/thesis-code/data/credit_card_clients.xls\")\n",
    "print(df['default payment next month'].value_counts())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jui/.pyenv/versions/3.10.12/envs/myenv/lib/python3.10/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    18112\n",
      "1    18112\n",
      "Name: SEX, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from raimitigations.dataprocessing import Rebalance  \n",
    "\n",
    "df['EDUCATION'] = df['EDUCATION'].replace({1: 1, 2: 1, 3: 2, 4: 3})\n",
    "\n",
    "bins = [0, 25, 35, 45, 55, 70, float('inf')]  # Define bin edges\n",
    "labels = [1, 2, 3, 4, 5, 6]  # Assign category labels\n",
    "\n",
    "df['AGE'] = pd.cut(df['AGE'], bins=bins, labels=labels, right=False).astype(int)\n",
    "\n",
    "df['MARRIAGE'] = df['MARRIAGE'].replace({0: 3})\n",
    "\n",
    "df = df.drop(columns=['ID'])\n",
    "df = df.drop(columns=['LIMIT_BAL'])\n",
    "\n",
    "# Count occurrences of each unique value across all PAY_* columns\n",
    "value_counts = df[['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']].stack().value_counts()\n",
    "\n",
    "# List of columns to modify\n",
    "pay_columns = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
    "\n",
    "# Replace -2 with -1\n",
    "df[pay_columns] = df[pay_columns].replace({-2:-1})\n",
    "\n",
    "rebalance = Rebalance(\n",
    "\t\t\t\tdf=df,\n",
    "\t\t\t\trebalance_col='SEX',\n",
    "\t\t\t\tk_neighbors=5,\n",
    "\t\t\t\tverbose=False\n",
    "\t\t\t)\n",
    "df = rebalance.fit_resample()\n",
    "print(df['SEX'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = 'default payment next month'\n",
    "\n",
    "def split_label(dataset, target_feature):\n",
    "    X = dataset.drop([target_feature], axis=1)\n",
    "    y = dataset[[target_feature]]\n",
    "    return X, y\n",
    "\n",
    "# Handle different scikit-learn versions for OneHotEncoder parameters\n",
    "if version.parse(sklearn.__version__) < version.parse('1.2'):\n",
    "    ohe_params = {\"sparse\": False}\n",
    "else:\n",
    "    ohe_params = {\"sparse_output\": False}\n",
    "\n",
    "def create_classification_pipeline(X,y):\n",
    "    pipe_cfg = {\n",
    "        'num_cols': X.select_dtypes(include=['int64', 'float64', 'float32']).columns.tolist(),\n",
    "        'cat_cols': X.select_dtypes(include=['object']).columns.tolist(),\n",
    "    }\n",
    "    \n",
    "    num_pipe = Pipeline([ \n",
    "        ('num_imputer', SimpleImputer(strategy='median')),\n",
    "        ('num_scaler', StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        ('cat_imputer', SimpleImputer(strategy='constant', fill_value='?')),\n",
    "        ('cat_encoder', OneHotEncoder(handle_unknown='ignore', **ohe_params))\n",
    "    ])\n",
    "    feat_pipe = ColumnTransformer([\n",
    "        ('num_pipe', num_pipe, pipe_cfg['num_cols']),\n",
    "        ('cat_pipe', cat_pipe, pipe_cfg['cat_cols'])\n",
    "    ])\n",
    "    \n",
    "    # Using XGBClassifier with Regularization, Learning Rate, and Eval Metrics\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        tree_method='hist',  # Fast histogram-based training\n",
    "        random_state=10,\n",
    "        n_jobs=-1,  # Use all CPU cores\n",
    "        learning_rate=0.16,  # Controls step size\n",
    "        reg_lambda=1.1,  # L2 regularization (weight decay)\n",
    "        eval_metric='logloss',\n",
    "        objective='binary:logistic',  # Logarithmic loss for classification\n",
    "        n_estimators=125,\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', feat_pipe),\n",
    "                               ('model', xgb_model)])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR-AUC: 0.5775005059302372\n",
      "Model Accuracy: 0.8419\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91      7305\n",
      "           1       0.66      0.37      0.47      1751\n",
      "\n",
      "    accuracy                           0.84      9056\n",
      "   macro avg       0.76      0.66      0.69      9056\n",
      "weighted avg       0.82      0.84      0.82      9056\n",
      "\n",
      "\n",
      "Final Training Log Loss: 0.3026\n",
      "Final Test Log Loss: 0.3941\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Load the saved train and test datasets from the files\n",
    "with open('train_data.pkl', 'rb') as f:\n",
    "    X_train, y_train = pickle.load(f)\n",
    "\n",
    "with open('test_data.pkl', 'rb') as f:\n",
    "    X_test, y_test = pickle.load(f)\n",
    "\n",
    "y_train = y_train.squeeze() \n",
    "\n",
    "# Create the classification pipeline\n",
    "pipeline = create_classification_pipeline(X_train, y_train)\n",
    "\n",
    "# Fit the preprocessor separately to extract feature names\n",
    "pipeline.named_steps['preprocessor'].fit(X_train)\n",
    "\n",
    "# Extract transformed feature names safely\n",
    "if hasattr(pipeline.named_steps['preprocessor'], \"get_feature_names_out\"):\n",
    "    feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "else:\n",
    "    # Manually construct feature names (for older sklearn versions)\n",
    "    num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    cat_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    feature_names = num_cols + cat_cols  # Not perfect, but works if get_feature_names_out() is missing\n",
    "\n",
    "# Convert y_train and y_test to NumPy arrays\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# Extract XGBClassifier separately and fit with eval_set\n",
    "xgb_model = pipeline.named_steps['model']\n",
    "model_improved = xgb_model.fit(pipeline.named_steps['preprocessor'].transform(X_train), y_train, \n",
    "              eval_set=[(pipeline.named_steps['preprocessor'].transform(X_train), y_train),\n",
    "                        (pipeline.named_steps['preprocessor'].transform(X_test), y_test)], verbose=False)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_improved = model_improved.predict(pipeline.named_steps['preprocessor'].transform(X_test))\n",
    "\n",
    "# Compute Precision-Recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_improved)\n",
    "\n",
    "# Calculate PR-AUC (area under the Precision-Recall curve)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "print(\"PR-AUC:\", pr_auc)\n",
    "\n",
    "# Compute accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred_improved)\n",
    "class_report = classification_report(y_test, y_pred_improved)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Classification Report:\\n{class_report}\\n\")\n",
    "\n",
    "# Extract evaluation results\n",
    "evals_result = xgb_model.evals_result()\n",
    "\n",
    "# Get the final log loss for training and testing\n",
    "train_log_loss = evals_result['validation_0']['logloss'][-1]\n",
    "test_log_loss = evals_result['validation_1']['logloss'][-1]\n",
    "\n",
    "# Print the final log loss values for both training and validation\n",
    "print(f\"Final Training Log Loss: {train_log_loss:.4f}\")\n",
    "print(f\"Final Test Log Loss: {test_log_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
