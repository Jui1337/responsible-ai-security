{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_excel(\"/home/jui/thesis-code/data/credit_card_clients.xls\")\n",
    "\n",
    "# Display the first few rows of the dataframe to verify it loaded correctly\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raimitigations.dataprocessing import Rebalance \n",
    "\n",
    "df['EDUCATION'] = df['EDUCATION'].replace({1: 1, 2: 1, 3: 2, 4: 3})\n",
    "bins = [0, 25, 35, 45, 55, 70, float('inf')]  # Define bin edges\n",
    "labels = [1, 2, 3, 4, 5, 6]  # Assign category labels\n",
    "\n",
    "df['AGE'] = pd.cut(df['AGE'], bins=bins, labels=labels, right=False).astype(int)\n",
    "df['MARRIAGE'] = df['MARRIAGE'].replace({0: 3})\n",
    "df = df.drop(columns=['ID'])\n",
    "\n",
    "\n",
    "\n",
    "rebalance = Rebalance(\n",
    "\t\t\t\tdf=df,\n",
    "\t\t\t\trebalance_col='SEX',\n",
    "\t\t\t\tk_neighbors=6,\n",
    "\t\t\t\tverbose=False\n",
    "\t\t\t)\n",
    "df = rebalance.fit_resample()\n",
    "print(df['SEX'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import xgboost as xgb\n",
    "from packaging import version\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def split_label(dataset, target_feature):\n",
    "    X = dataset.drop([target_feature], axis=1)\n",
    "    y = dataset[[target_feature]]\n",
    "    return X, y\n",
    "\n",
    "# Handle different scikit-learn versions for OneHotEncoder parameters\n",
    "if version.parse(sklearn.__version__) < version.parse('1.2'):\n",
    "    ohe_params = {\"sparse\": False}\n",
    "else:\n",
    "    ohe_params = {\"sparse_output\": False}\n",
    "\n",
    "def create_classification_pipeline(X):\n",
    "    pipe_cfg = {\n",
    "        'num_cols': X.dtypes[X.dtypes == 'int64'].index.values.tolist(),\n",
    "        'cat_cols': X.dtypes[X.dtypes == 'object'].index.values.tolist(),\n",
    "    }\n",
    "    num_pipe = Pipeline([ \n",
    "        ('num_imputer', SimpleImputer(strategy='median')),\n",
    "        ('num_scaler', StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        ('cat_imputer', SimpleImputer(strategy='constant', fill_value='?')),\n",
    "        ('cat_encoder', OneHotEncoder(handle_unknown='ignore', **ohe_params))\n",
    "    ])\n",
    "    feat_pipe = ColumnTransformer([\n",
    "        ('num_pipe', num_pipe, pipe_cfg['num_cols']),\n",
    "        ('cat_pipe', cat_pipe, pipe_cfg['cat_cols'])\n",
    "    ])\n",
    "\n",
    "    # Using XGBClassifier with Regularization, Learning Rate, and Eval Metrics\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        tree_method='hist',  # Fast histogram-based training\n",
    "        random_state=10,\n",
    "        n_jobs=-1,  # Use all CPU cores\n",
    "        learning_rate=0.17,  # Controls step size\n",
    "        reg_lambda=1.0,  # L2 regularization (weight decay)\n",
    "        eval_metric='logloss',  # Logarithmic loss for classification\n",
    "        use_label_encoder=False,  # Avoids unnecessary warnings\n",
    "        n_estimators=700,\n",
    "        early_stopping_rounds=65\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', feat_pipe),\n",
    "                               ('model', xgb_model)])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = 'default payment next month'\n",
    "categorical_features = []\n",
    "\n",
    "# Split data into features and target\n",
    "X, y = split_label(df, target_feature)\n",
    "\n",
    "# Split data into train and test sets \n",
    "X_train_og, X_test_og, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=40)\n",
    "\n",
    "# Create the classification pipeline\n",
    "pipeline = create_classification_pipeline(X_train_og)\n",
    "\n",
    "# Fit the preprocessor separately to extract feature names\n",
    "pipeline.named_steps['preprocessor'].fit(X_train_og)\n",
    "\n",
    "# Extract transformed feature names safely\n",
    "if hasattr(pipeline.named_steps['preprocessor'], \"get_feature_names_out\"):\n",
    "    feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "else:\n",
    "    # Manually construct feature names (for older sklearn versions)\n",
    "    num_cols = X_train_og.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    cat_cols = X_train_og.select_dtypes(include=['object']).columns.tolist()\n",
    "    feature_names = num_cols + cat_cols  # Not perfect, but works if get_feature_names_out() is missing\n",
    "\n",
    "# Convert y_train and y_test to NumPy arrays\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# Extract XGBClassifier separately and fit with eval_set\n",
    "xgb_model = pipeline.named_steps['model']\n",
    "model = xgb_model.fit(pipeline.named_steps['preprocessor'].transform(X_train_og), y_train, \n",
    "              eval_set=[(pipeline.named_steps['preprocessor'].transform(X_train_og), y_train),\n",
    "                        (pipeline.named_steps['preprocessor'].transform(X_test_og), y_test)], verbose=False)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(pipeline.named_steps['preprocessor'].transform(X_test_og))\n",
    "\n",
    "# Compute accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Classification Report:\\n{class_report}\\n\")\n",
    "\n",
    "# Extract evaluation results\n",
    "evals_result = xgb_model.evals_result()\n",
    "\n",
    "# Get the final log loss for training and testing\n",
    "train_log_loss = evals_result['validation_0']['logloss'][-1]\n",
    "test_log_loss = evals_result['validation_1']['logloss'][-1]\n",
    "\n",
    "# Print the final log loss values for both training and validation\n",
    "print(f\"Final Training Log Loss: {train_log_loss:.4f}\")\n",
    "print(f\"Final Test Log Loss: {test_log_loss:.4f}\")\n",
    "\n",
    "# Plot training and validation log loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(evals_result['validation_0']['logloss'], label='Train Log Loss', color='blue')\n",
    "plt.plot(evals_result['validation_1']['logloss'], label='Test Log Loss', color='red')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Log Loss\")\n",
    "plt.title(\"XGBoost Training Progress (Log Loss)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raiwidgets import ResponsibleAIDashboard\n",
    "from responsibleai import RAIInsights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibleai.feature_metadata import FeatureMetadata\n",
    "# Set up feature metadata for RAIInsights\n",
    "feature_metadata = FeatureMetadata(categorical_features=categorical_features, dropped_features=[])\n",
    "\n",
    "# Add the target feature back to the datasets\n",
    "X_train_og_with_target = X_train_og.copy()\n",
    "X_train_og_with_target[target_feature] = y_train\n",
    "\n",
    "X_test_og_with_target = X_test_og.copy()\n",
    "X_test_og_with_target[target_feature] = y_test\n",
    "X_test_og_with_target = X_test_og_with_target.sample(n=1000, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, pass these modified DataFrames to RAIInsights\n",
    "rai_insights = RAIInsights(model, X_train_og_with_target, X_test_og_with_target, target_feature, 'classification', feature_metadata=feature_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretability\n",
    "rai_insights.explainer.add()\n",
    "# Error Analysis\n",
    "rai_insights.error_analysis.add()\n",
    "# Counterfactuals: accepts total number of counterfactuals to generate, the label that they should have, and a list of \n",
    "                # strings of categorical feature names\n",
    "rai_insights.counterfactual.add(total_CFs=10, desired_class='opposite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute: Perform all tasks (this remains CPU-bound)\n",
    "rai_insights.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResponsibleAIDashboard(rai_insights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
