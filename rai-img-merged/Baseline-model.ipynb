{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"/home/jui/thesis-code/data/credit_card_clients.xls\")\n",
    "\n",
    "# Display the first few rows of the dataframe to verify it loaded correctly\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import xgboost as xgb\n",
    "from packaging import version\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def split_label(dataset, target_feature):\n",
    "    X = dataset.drop([target_feature], axis=1)\n",
    "    y = dataset[[target_feature]]\n",
    "    return X, y\n",
    "\n",
    "# Handle different scikit-learn versions for OneHotEncoder parameters\n",
    "if version.parse(sklearn.__version__) < version.parse('1.2'):\n",
    "    ohe_params = {\"sparse\": False}\n",
    "else:\n",
    "    ohe_params = {\"sparse_output\": False}\n",
    "\n",
    "def create_classification_pipeline(X):\n",
    "    pipe_cfg = {\n",
    "        'num_cols': X.dtypes[X.dtypes == 'int64'].index.values.tolist(),\n",
    "        'cat_cols': X.dtypes[X.dtypes == 'object'].index.values.tolist(),\n",
    "    }\n",
    "    num_pipe = Pipeline([ \n",
    "        ('num_imputer', SimpleImputer(strategy='median')),\n",
    "        ('num_scaler', StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        ('cat_imputer', SimpleImputer(strategy='constant', fill_value='?')),\n",
    "        ('cat_encoder', OneHotEncoder(handle_unknown='ignore', **ohe_params))\n",
    "    ])\n",
    "    feat_pipe = ColumnTransformer([\n",
    "        ('num_pipe', num_pipe, pipe_cfg['num_cols']),\n",
    "        ('cat_pipe', cat_pipe, pipe_cfg['cat_cols'])\n",
    "    ])\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        tree_method='hist',  # Fast histogram-based training\n",
    "        random_state=10,\n",
    "        n_jobs=-1,  # Use all CPU cores\n",
    "        learning_rate=0.17,  # Controls step size\n",
    "        reg_alpha=0.65,  # L1 regularization (sparsity)\n",
    "        reg_lambda=1.0,  # L2 regularization (weight decay)\n",
    "        eval_metric='logloss',  # Logarithmic loss for classification\n",
    "        use_label_encoder=False,  # Avoids unnecessary warnings\n",
    "        n_estimators=700,\n",
    "        early_stopping_rounds=65\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', feat_pipe),\n",
    "                               ('model', xgb_model)])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = 'default payment next month'\n",
    "categorical_features = []\n",
    "\n",
    "# Split data into features and target\n",
    "X, y = split_label(df, target_feature)\n",
    "\n",
    "# Split data into train and test sets (80% training, 20% testing)\n",
    "X_train_og, X_test_og, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=40)\n",
    "\n",
    "# Create the classification pipeline\n",
    "pipeline = create_classification_pipeline(X_train_og)\n",
    "\n",
    "# Fit the preprocessor separately to extract feature names\n",
    "pipeline.named_steps['preprocessor'].fit(X_train_og)\n",
    "\n",
    "# Extract transformed feature names safely\n",
    "if hasattr(pipeline.named_steps['preprocessor'], \"get_feature_names_out\"):\n",
    "    feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "else:\n",
    "    # Manually construct feature names (for older sklearn versions)\n",
    "    num_cols = X_train_og.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    cat_cols = X_train_og.select_dtypes(include=['object']).columns.tolist()\n",
    "    feature_names = num_cols + cat_cols  # Not perfect, but works if get_feature_names_out() is missing\n",
    "\n",
    "# Convert y_train and y_test to NumPy arrays\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# Extract XGBClassifier separately and fit with eval_set\n",
    "xgb_model = pipeline.named_steps['model']\n",
    "model = xgb_model.fit(pipeline.named_steps['preprocessor'].transform(X_train_og), y_train, \n",
    "              eval_set=[(pipeline.named_steps['preprocessor'].transform(X_train_og), y_train),\n",
    "                        (pipeline.named_steps['preprocessor'].transform(X_test_og), y_test)], verbose=False)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(pipeline.named_steps['preprocessor'].transform(X_test_og))\n",
    "\n",
    "# Compute accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Classification Report:\\n{class_report}\\n\")\n",
    "\n",
    "# Extract evaluation results\n",
    "evals_result = xgb_model.evals_result()\n",
    "\n",
    "# Get the final log loss for training and testing\n",
    "train_log_loss = evals_result['validation_0']['logloss'][-1]\n",
    "test_log_loss = evals_result['validation_1']['logloss'][-1]\n",
    "\n",
    "# Print the final log loss values for both training and validation\n",
    "print(f\"Final Training Log Loss: {train_log_loss:.4f}\")\n",
    "print(f\"Final Test Log Loss: {test_log_loss:.4f}\")\n",
    "\n",
    "# Plot training and validation log loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(evals_result['validation_0']['logloss'], label='Train Log Loss', color='blue')\n",
    "plt.plot(evals_result['validation_1']['logloss'], label='Test Log Loss', color='red')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Log Loss\")\n",
    "plt.title(\"XGBoost Training Progress (Log Loss)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
